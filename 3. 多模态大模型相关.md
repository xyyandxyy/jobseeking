# 多模态大模型相关

# RoPE

$$
f(q, m)=R_m q=\left(\begin{array}{cc}
\cos m \theta & -\sin m \theta \\
\sin m \theta & \cos m \theta
\end{array}\right)\binom{q_0}{q_1}
$$

RoPE位置编码通过将k和q左乘一个旋转矩阵，为其赋予位置信息.  然后Q，K再进行接下来的注意力分数矩阵的计算，总的来说LLaMA对于Rope的实现是通过复数乘法运算.

其中m为位置下标, $\theta$与维度k和位置m有关:
$$
\theta_k=\frac{m}{10000^{\frac{2 k}{d}}}
$$
d是词嵌入向量的总维度

然后存在以下性质, 以实现相对位置编码:
$$
\left(\boldsymbol{\mathcal { R }}_m \boldsymbol{q}\right)^{\top}\left(\boldsymbol{\mathcal { R }}_n \boldsymbol{k}\right)=\boldsymbol{q}^{\top} \boldsymbol{\mathcal { R }}_m^{\top} \boldsymbol{\mathcal { R }}_n \boldsymbol{k}=\boldsymbol{q}^{\top} \boldsymbol{\mathcal { R }}_{n-m} \boldsymbol{k}
$$

***

RoPE具有远程衰减性

***

在 RoPE 设计中，低维度的部分会有较高的旋转频率，而高维度的部分旋转频率则较低。频率的高低决定了嵌入向量如何与序列位置（token 位置）进行编码。具体来说：

- **低维度编码：高频信息**

低维度部分会有较大的旋转角度，因此这些维度在嵌入中会捕捉到与位置变化相关的高频率信息。这意味着这些维度在较短的序列之间会更敏感，能够更好地捕捉到位置较为接近的 token 之间的差异，也就是**短距离依赖**（近邻 token）。

- **高维度编码：低频信息**

高维度部分由于旋转角度非常小，它的频率更低，这意味着高维度部分的嵌入会更适合编码**长距离依赖**（远程 token）。也就是说，这些维度对 token 的位置变化不敏感，但更擅长表示长距离 token 之间的相对位置信息。

## 为什么KV Cache 不用对Q做Cache

| 阶段             | 全序列推理      | 增量推理（单步新 token）                                   |
| ---------------- | --------------- | ---------------------------------------------------------- |
| 输入 embedding   | (B, T, d_model) | 历史：cached: (B, T, d_model)<br>新 token: (B, 1, d_model) |
| Q                | (B, T, d_model) | 仅新 token Q_new: (B, 1, d_model)                          |
| K                | (B, T, d_model) | 缓存后 keys: (B, T+1, d_model)                             |
| V                | (B, T, d_model) | 缓存后 values: (B, T+1, d_model)                           |
| Attention scores | (B, T, T)       | (B, 1, T+1)                                                |
| 输出             | (B, T, d_model) | (B, 1, d_model)                                            |

这样可以看出，全序列推理每一步都会对整个序列计算 Q、K、V，并产生对应 T × T 的注意力计算；而增量推理中，只对当前新 token 计算 Q，并利用缓存的历史 K、V，使得每步只需计算 (1 × (T+1)) 的注意力得分，从而大幅节省不必要的重复计算。

────────────────────────  
【全序列推理的情况】  
────────────────────────  
假设输入的序列长度为 T，输出 output 的维度为 (B, T, d_model)。  
在这种情况下，模型一次性计算了整个序列所有 token 的输出。如果目标是生成下一个 token，对于已知输入序列，可以直接取输出序列中最后一个 token 对应的隐藏状态（也就是 output[:, T-1, :]）：

1. 取最后时刻的输出：  
   last_output = output[:, -1, :]    维度 (B, d_model)  

2. 将 last_output 经过一个输出投影层（通常是一个线性层）转换为词汇表大小 logits，再加上 softmax 得到概率分布，最终选择概率最大（或通过采样方式）的 token 作为下一个 token。  

这种方式适用于一次性输入整个序列（例如训练或批量生成）时，从全序列输出中选取最后一个位置代表未来要生成的 token。

────────────────────────  
【增量推理的情况】  
────────────────────────  
在增量生成中，每一步只对最新的 token 计算新的输出，因此 output 的维度是 (B, 1, d_model)，表示当前这一步的预测表示。  
生产下一个 token的过程如下：

1. 当前步骤输出：  
   new_output = output    维度 (B, 1, d_model)  
                     （这里 new_output 表示当前新生成 token 在模型中经过注意力计算后的表示。）

2. 直接将 new_output squeeze 为 (B, d_model)，再经过与词汇表维度对应的线性变换以及 softmax，得到下一个 token 的预测概率分布。  

这种方式中，因为只有当前新 token 的输出，所以直接用这一输出表示来预测结果，无需再进行切片选择。

────────────────────────  
【总结对比】  

| 实现方式     | output 维度     | 如何获得预测 token                               |
| ------------ | --------------- | ------------------------------------------------ |
| 全序列推理版 | (B, T, d_model) | 取最后一个位置 output[:, -1, :]，然后变换 logits |
| 增量推理版   | (B, 1, d_model) | 新 token 的输出即 new_output，直接变换 logits    |

## 训练和推理时的区别

在许多主流 Transformer 模型（比如 Llama）的实际实现中，训练过程和推理过程在生成 token 的方式上存在明显的区别，主要有以下几点：

1. 训练过程采用全序列并行计算  
   • 在训练时，通常采用“Teacher Forcing”策略，将一个完整的输入序列一次性传入模型。  
                     • 模型会同时计算序列中所有位置的输出表示，并利用自注意力中的掩码（mask）来阻止每个位置“看到”未来的信息。  
                     • 损失函数通常对序列中每个 token 的预测进行监督，即对每个 token 生成对应的输出（除了起始 token）并计算交叉熵损失。

2. 推理过程采用自回归逐步生成  
   • 在推理阶段，模型以递归的方式一步步生成 token。  
                     • 每一步只接收新生成的 token（或者通过缓存机制只输入新的 token 的 embedding），然后只计算当前新 token 的输出表示。  
                     • 最后，只利用当前步骤输出的最后一层隐藏状态经过线性变换映射到词汇表大小的 logits，从而获得下一个 token 的预测。

总结来说：

- 训练时：  
  • 目标是并行地获取整序列每个位置的预测结果  
                    • 使用掩码确保历史信息和未来信息的正确分割  
                    • 输出 shape 为 (B, T, d_model)，对所有 token 计算损失

- 推理时：  
  • 仅关注当前新 token 的生成  
                    • 通过缓存机制避免重复计算历史 token 的 Key 和 Value，只计算新 token 的 Query  
                    • 输出 shape 通常为 (B, 1, d_model)，直接用于预测下一个 token

这两种模式下的计算方式是不同的，但核心思想都是基于自注意力机制，只不过训练时为了并行计算和更高效的学习，需要对整个序列进行一次性处理，同时加入遮掩（mask）处理，而推理时则为了计算效率和避免重复计算，只关注当前生成的 token 输出。

## 介绍一下BLIP2和Q-Former

Blip2的Adapter包含:

- Query-Transformer  (Q-Former)
- 一层Linear

**推理时：**

1. **将img输入到Q-Former中**
2. **利用Cross Attention 将vision emb 与learnable querys (序列长度较短) 进行交互。**
3. **将交互后的learnable querys作为视觉特征，输入到大模型**

> **从而达到降低视觉特征序列长度的目的。**可以有效地充当`信息瓶颈`，将最有用的信息提供给 LLM，同时删除不相关的视觉信息



------

<img src="pic/640" alt="Image" style="zoom:50%;" />

Q-Former 包含两个Transformer, 但是其中的self-attention是共享的.

1. 一个image Transformer. 有self-attn, FFN, 还有一个cross-attn
2. 一个text Transformer. 只有self-attn和FFN

Q-Former是从BERT初始化的, 所以确实就是encoder架构, 不过在self-attn和FFN之间加了cross-attn

<img src="pic/image-20250225200351265.png" alt="image-20250225200351265" style="zoom:50%;" />

<img src="pic/image-20250225200412420.png" alt="image-20250225200412420" style="zoom:50%;" />

第一阶段训练任务:

1. ITC(Image-Text Contrastive Learning)
   对齐图像和文本的embedding. 采用单模态自注意力掩码, 不允许Query和Text相互关注
2. ITG(Image-grounded Text Generation)
   在给定输入图像作为条件的情况下，训练 Q-Former 生成文本(直接作为解码器, 而不用后面的LLM)，迫使Query提取包含文本信息的视觉特征。

3. ITM( Image-Text Matching)
   预测图像-文本对是正匹配还是负匹配，学习图像和文本表示之间的细粒度对齐。使用双向自注意掩码，所有Query和Text都可以相互关注。

第二阶段训练任务:

1. 在生成预训练阶段，将 Q-Former连接到冻结的 LLM，以利用 LLM 的语言生成能力。这里**使用全连接层**将输出的Query嵌入线性投影到与 LLM 的文本嵌入相同的维度，然后将投影的Query嵌入添加到输入文本嵌入前面由于 Q-Former 已经过预训练，可以提取包含语言信息的视觉表示，因此它可以有效地充当信息瓶颈，将最有用的信息提供给 LLM，同时删除不相关的视觉信息，减轻了 LLM 学习视觉语言对齐的负担。

## **LLaVA**

论文发布时间：  

- 2023年4月（初版）  
- 2023年12月（更新）

模型结构：  

- **视觉编码器（Vision Encoder）**：预训练的 CLIP visual encoder ViT-L/14（分辨率 224×224）  
- **语言大模型（LLM）**：Vicuna  
- **投影层（Projection）**：一个简单的线性层，用于将图像特征映射到词嵌入空间

不同于 Flamingo 的 gated cross-attention 和 BLIP-2 的 Q-former，LLaVA 的映射层较为简单，便于以数据为中心进行实验迭代。

训练方法：  

- **阶段一：特征对齐的预训练（Pre-training for Feature Alignment）**  
  - 冻结视觉编码器和 LLM，只训练投影层。  
  - 选取了 CC3M 数据集中过滤后的 595K 图文对，并将这些图文对改造为指令跟随数据（每个样本构成一个单轮对话），目标输出为图像的原始描述文本。  
  - 本阶段实质上为冻结的 LLM 训练了一个兼容的视觉 tokenizer，使得视觉编码器输出的图像特征可以和预训练 LLM 的词向量对齐。  
  - 训练配置：  
    - 数据集：CC-595K 过滤后的子数据集  
    - Epoch 数：1  
    - 学习率：2e-3  
    - Batch_size：128

- **阶段二：端到端微调（Fine-tuning End-to-End）**  
  - 冻结视觉编码器，训练投影层和 LLM。  
  - 两个实验设置：  
    - **多模态对话机器人**：  
      - 使用 158K 的语言-图像指令跟随数据对 Chatbot 进行微调开发。  
      - 样本类型包括：  
        - 对话型（多轮交互式）  
        - 其他两种单轮交互式回答  
      - 在训练过程中，这些样本被均匀采样使用。  
    - **ScienceQA 数据集**：  
      - ScienceQA 包含 21,000 个多模态多项选择题，涵盖广泛领域（3 个学科、26 个主题、127 个类别、379 项技能）。  
      - 数据集划分：  
        - 训练集：12,726 个样本  
        - 验证集：4,241 个样本  
        - 测试集：4,241 个样本  
      - 训练配置：  
        - 基于 LLaVA-Instruct-158K 数据集  
        - Epoch 数：3  
        - 学习率：2e-5  
        - Batch_size：32

---

## **LLaVA-1.5（通过视觉指令微调改进基线性能）**

论文发布时间：2023年10月

基于 LLaVA 的改动点：  

- 明确指定输出格式的提示  

  - 为解决短文本 VQA 和长文本 VQA 输出兼容问题，在短文本回答中明确指定输出格式提示。  
  - 例如，在问题文本末尾添加 “Answer the question using a single word or phrase”，使模型根据用户指示适当调整输出格式。

- 使用两层 MLP 作为视觉-语言连接器  

  - 相比 LLaVA 的单个线性投影层，两层 MLP 能够增强连接器的表达能力，从而显著提升多模态能力。

- 添加特定任务的数据集  

  - 为增强模型在不同能力上的表现，不仅增加 VQA 数据集，同时专注于 OCR 以及区域级别识别的四个数据集。  
  - 数据集示例：  
    - 需要广泛知识的 VQA（如 OKVQA 和 A-OKVQA）  
    - 需要 OCR 的 VQA（如 OCRVQA 和 TextCaps）

- 规模扩大  

  - 图片分辨率提升至 336（通过将视觉编码器替换为 CLIPViT-L-336px）。

- 引入 anyres  

  - 痛点：不论使用 ViT-L/14-224px 还是 ViT-L/14-336px，都只能支持固定分辨率的图像作为输入。  

  - 目标：支持任意高分辨率的图像。  

  - 核心思路：将图片切分成多个 patch，分别获得 patch 的图像特征。  

  - 方法：  

    - split原图：将一张图片切分成若干个 patch，单独对每个 patch 进行 encoder 编码，组合为一个更大的图片特征。  
    - resize原图：将图片 resize 到固定尺寸 336×336，作为全局上下文（Global context）加入到图片特征中。

    ![img](pic/v2-cc69ae3416f3596f04257474ec8aa565_1440w.jpg)

- 新增数据集  

  - 加入 GQA 数据集，作为额外的图片知识。  
  - 加入 ShareGPT 数据。  
  - 扩大 LLM 至 13B 参数。  
  - 在 MM-Vet 上的结果表明，当 LLM 扩展至 130 亿参数时，改进最为显著，说明基础 LLM 的能力对于视觉对话至关重要。

---

## **LLaVA 1.6 (LLaVA-NeXT)**

改进相较于 LLaVA-1.5：  

- 推理、OCR 和世界知识方面的显著提升  
  - 在多个基准测试上超过 Gemini Pro 的表现。

- 图像输入改进  
  - 将输入图像分辨率提高 4 倍，以捕捉更多视觉细节。  
  - 支持三种不同的宽高比，最高可达：  
    - 672×672  
    - 336×1344  
    - 1344×336

- 微调数据混合改进  
  - 通过改进视觉指令微调数据混合，进一步提升视觉推理和 OCR 能力。

- 扩展视觉对话能力  
  - 针对更多场景优化视觉对话功能，涵盖不同应用领域，同时增强了模型的世界知识和逻辑推理能力。

- 部署与推理  
  - 采用 SGLang 实现高效部署与推理。  

- 数据效率  
  - LLaVA-NeXT 重用了 LLaVA-1.5 预先训练的连接器，并且仅使用了少于 100 万的视觉指令微调样本。  

- 训练配置示例  
  - 最大版（340 亿参数）在 32 块 A100 显卡上，约 1 天即可完成训练。





## Qwen-VL

基本架构

1. LLM：使用了来自Qwen-7B模型的预训练权重
2. 视觉编码器：采用了Vision Transformer（ViT）架构，用于处理输入图像并生成一组图像特征。在训练和推理过程中，将输入图像调整到特定的分辨率，然后通过将图像分割成大小为14的图块来处理它们。预训练权重来自于OpenAI的clip的[ViT-bigG](https://zhida.zhihu.com/search?content_id=249031429&content_type=Article&match_order=1&q=ViT-bigG&zhida_source=entity)。
3. 位置感知的视觉-语言适配器（Position-aware Vision-Language Adapter）：为了缩短图像token长度，同时作模态映射的模块。由一个单层的交叉注意力模块构成，使用一组可训练的向量（嵌入）作为查询向量，使用来自视觉编码器的图像特征作为交叉注意力操作的键。随机初始化。

位置感知的视觉-语言适配器（Position-aware Vision-Language Adapter）
作用
该组件的作用在于使得长序列的图像特征变得更加紧凑，从而提高了处理效率。同时，考虑到位置信息可以帮助模型更准确地理解图像细节，因此在压缩过程中保留了这方面的信息。

实现
该适配器由一个单层交叉注意力模块组成。 模块使用一组可训练的向量（嵌入）作为查询向量，使用来自视觉编码器的图像特征作为键进行交叉注意力操作。 将图像特征序列压缩成了固定长度的256。 考虑到图像中位置信息的重要性，作者引入了2D绝对位置编码到交叉注意力机制的查询-键对中。

stage1：预训练，目标是使用大量的图文对数据对齐视觉模块和LLM的模态，这个阶段冻结LLM模块的参数； stage2：多任务预训练，使用更高质量的图文多任务数据（主要来源自开源VL任务，部分自建数据集），更高的图片像素输入，全参数训练； stage3：指令微调阶段，这个阶段冻结视觉Encoder模块，使用的数据主要来自大模型Self-Instruction方式自动生成，目标是提升模型的指令遵循和多轮对话能力。

***

**之前的多模态大模型通常是基于单个图片做多轮对话，且不能给出图中某个物体的具体位置**。QWenVL更改了模型的输入输出格式，并用标记语言(类似html)和Tokenizer来适配多图多轮对话和目标检测任务。Assistant后面为语言模型需要生成的内容，其他内容均为Prompt。为了方便读者理解，下述例子与QWenVL的真实输入输出略有出入。

```
其他多模态大模型的输入输出格式, "..."表示图1中的aligned img emb
<img>...</img>
User:请给出图中的动物的种类
Assistant:熊猫
User:请给出图中共有多少只熊猫
Assistant:5个

QWenVL中适配多张图片的输入输出格式
User: Picture 1<img>img_path_1</img>, 请给出图中动物的种类
Assistant:老虎
User: Picture 2<img>img_path_2</img>, 请判断上述两张图中共有多少只动物
Assistant:10只
User: 请给出图1中老虎的位置
Assistant:<ref>老虎</ref><box>(10,12),(120,130)</box>
```

QwenVL的Tokenizer在接受Prompt的时候会自动识别Prompt中表示图像地址的部分，然后将该地址pad到固定长度256。在模型进行forward的时候，会识别图片地址的位置并通过图1中的Image Encoder和Adapter将图片转为长度为256的序列(aligned img emb)，并将该序列替换掉Prompt中的图片地址。

<img src="pic/640-1793916." alt="Image" style="zoom:33%;" />

QWenVL还能给出图中物品的类别和坐标，表示方法为

```
<ref>类别</ref><box>(min_x,min_y),(max_x,max_y)</box>
```

QWenVL用特定格式的输出表示检测框位置和被检测物体的类别，其坐标直接使用Token表示，坐标是将图片进行等比例缩放直到最长边为1000后图片上的坐标。这种使用Token表示坐标的方式非常反直觉，但确实能Work。



QWenVL使用FC层和单层Cross Attention作为将视觉模态Adapter。 与之前介绍的Adapter不同，QWen先使用FC层将图像特征的维度对齐到大模型，再使用Cross Attention将图像特征的长度变为指定的长度，其先使用FC，再使用Cross Attention的做法可能是为了增加Adapter的参数量。

<img src="pic/640-20250306102933994" alt="Image" style="zoom:50%;" />

除此之外，QWenVL的Adapter还在图像的位置信息上做了一点调整。之前的多模态大模型使用的视觉Encoder通常是将图像的patch按从上到下，从左到右的顺序排序成一个序列(图2中的img emb)，并按照序列的顺序给图像加入位置信息emb(1D位置emb)。除此之外，此前介绍的多模态大模型并未关注图像emb的位置信息。

除了来自视觉模型的1D位置emb，QWenVL还在Adapter中的cross attn中加入了2D位置emb。其具体做法是将2D位置emb加到cross attn中的Key上，2D位置emb表示该patch是图像分块后的第几行和第几列，是通过分别计算行、列的1D位置emb再concat到一起得到的，其中行、列的1D位置emb来自sin位置emb。显然的，加入的2D位置emb对于不同长宽比的图片会更加友好，虽然QWenVL输入的图片有着固定的大小，但这可能是为后续的QWenVL-Plus/QWenVL-Max打下的基础。

**与LLaVA，Mini-GPT4不同QWenVL的训练被分为三个阶段**，分别是**特征对齐训练**，**能力增强训练**和**指令跟随训练**，不同阶段的模型可训练参数也各不相同。

<img src="pic/640-20250312233840795" alt="Image" style="zoom:33%;" />

**2.3.1 特征对齐训练**

特征对齐训练是使用大量的图文对数据(img-caption)将视觉Encoder对齐到LLM上去，让LLM能看到图片上的信息。其特点是数据量大，文字内容小(seq_len较小，256图像 + 512LLM)，训练速度快(因为seq_len小，所以训练速度相对较快)。QWenVL从各种开源数据集中收集了1.4B的数据，其中77.3%为英文数据，22.7%为中文数据，具体数据来源见表1。

训练时图像被缩放至224*224的大小，batch-size 为30720，训练了50K步，训练时Vision Encoder和Adapter的权重有更新。其输入输出格式如下

```
User: Picture 1<img>tiger.jpg</img>
Assistant:5只老虎在大草原上捕猎
```

**2.3.2 能力增强训练**

在将Image Encoder对齐到大模型后，QWenVL将输入的图像大小从224 * 224 调整到 448 * 448，并使用了7个不同的训练任务来增强模型的能力。模型的全部参数均参与训练，训练的seq_len为2048。

**2.3.3 指令跟随训练**

该阶段主要是让模型拥有聊天的能力，模型开放Adapter和LLM的参数用于训练。QWenVL使用了基于LLM生成多模态的对话数据，并保证对话内容和图像内容相关，指令微调数据量为359K，论文中并未具体给出数据生成的方法。





## internVL-v1.5

模型上，internVL-v1.5首次将ImageEncoder(VIT)的参数量扩展到了6B，将图像的视觉特征（图1中的aligned img emb）扩展到了1.7k，将图像的分辨率提升到7个448 * 448的patch(后续会解释)，并引入了动态长宽比。数据上，InternVL-v1.5不仅收集了大量自然场景数据，还收集了大量文档数据，进一步提升算法的OCR能力。

|               | VIT的参数量 | 图像分辨率 | 视觉特征序列长度 | 动态长宽比 |
| ------------- | ----------- | ---------- | ---------------- | ---------- |
| 第一阶段模型  | <300M       | 448*448    | 512              | 不支持     |
| InternVL-V1.5 | 6B          | 448*448*7  | 1792             | 支持       |

***

动态分辨率

此前，多模态大模型通常是将图像resize到固定的长宽比(448 x 448 或 1344 x1344)，再送入VIT获取图像特征，这种做法会造成两个问题：

1. 对于极端长宽比的图片，失真严重
2. 若固定一个较大的长宽比（比如1344*1344），则VIT会产生超长的序列长度，消耗大量算力



<img src="pic/640-20250312233844930" alt="Image" style="zoom:67%;" />

动态长宽比的实现分为以下4个步骤：

1. 预设最大patch数量：max_patches，图像会被分为多个448x448的patch，但会保证patch数不超过max_patch，图2中max_patches被设为7
2. 根据最大patch数量给出能够适配的长宽比（长 * 宽 <= max_patch - 1），图2左下部分即为max_patches=7时候能够适配的部分长宽比
3. 根据原图分辨率和原图长宽比，给出最匹配的长宽比（图2中为 2:3），并将图片resize并分为多个448 x448的patch（图2中，想将图片从[800, 1300] resize成 [448x2 = 896, 448x3 =1344]，再将其切片成为 2x3=6个 448x 448 的patch）
4. 最后将原图resize到448x448放在patch的后面，就得到了图像经过动态长宽比后的图片（图2中部小图）
5. 这些小图分别经过VIT后会得到max_patch个1024长度的序列，将这些序列拼接起来即可得到图像序列

这样做的好处是

1. 可以处理较为极端长宽比的图像，避免了因resize导致的图像变形
2. 大分辨率图对应的序列长度较长，小分辨率图对应的序列长度较小
3. 将图片切成了多个448x448的小片过VIT，采用了分块的思想，解决了大分辨率图像计算量大的问题。





## Qwen2-vl

| 模型尺度                 | VIT模型大小固定在675M；LLM大小分为1.5B，7.6B和72B三个版本    |
| ------------------------ | ------------------------------------------------------------ |
| 图像预处理               | 不仅关注图像宽高，还关注图像分辨率。分辨率越高的图像，使用更多的视觉token来表示 |
| 统一的多模态Rope位置编码 | 开发了一套位置编码，用于表示视觉视频和文字的位置信息         |

<img src="pic/640-20250312233850589" alt="Image" style="zoom: 50%;" />

InternVL系列的图像转序列(img_emb)模块会将图像分为多个448*448的patch，即图像的长宽会被resize到448的倍数，该方法虽然能支持动态长宽比，但实际应用时，该图像分patch机制更多考虑的是长宽比，而非分辨率。

| 使用模型      | 图像分辨率 | resize后分辨率 | 图像序列长度 |
| ------------- | ---------- | -------------- | ------------ |
| InternVL-v1.5 | 512x512    | 448x448        | 256          |
| InternVL-v1.5 | 40x400     | 448x4480       | 2560         |
| Qwen2VL       | 512x512    | 504x504        | 324          |
| Qwen2VL       | 40x400     | 28x392         | 14           |

而Qwen2VL的图像转序列不仅考虑了长宽比还考虑了分辨率对img_emb序列长度的影响, 极大程度上提升了模型的infer效率。其实现动态分辨率的具体做法分为图像预处理和VIT两个部分。

**图像预处理部分：**以512x512的图像为例，该图像会被resize成504x504；然后被分为 36x36个patch，每个patch的大小均为14x14x3；随后channel维度进行重复得到  36x36个patch，每个patch的大小均为14x14x3x2；最后会对图像进行flatten操作，得到(36x36, 14x14x3x2) = （1296, 1176）的序列。

<img src="pic/640-20250312233852672" alt="Image" style="zoom:67%;" />



***

基本架构
LLM+视觉编码器结构，无adapter。 LLM：Qwen2 视觉编码器：实现了一种具有大约675百万参数的ViT，能够处理图像和视频输入。

原生动态分辨率 Naive Dynamic Resolution
Qwen2-VL现在可以处理任意分辨率的图像，动态将其转换为可变数量的视觉tokens。为支持此功能，修改了ViT，去除了原始的绝对位置嵌入，并引入了2D-RoPE，以捕捉图像的二维位置信息。

在推理阶段，不同分辨率的图像被打包成一个单一序列，打包长度受到控制，以限制GPU内存使用。此外，为减少每幅图像的视觉tokens，在ViT后采用了一个简单的多层感知器（MLP）层，将相邻的2×2个tokens压缩为一个token，并在压缩的视觉tokens的开头和结尾放置特殊的<|vision_start|>和<|vision_end|> tokens。因此，分辨率为224×224的图像在使用patch_size=14的ViT编码后，将在进入LLM之前压缩为66个tokens（16*16/4+2）。

训练
遵循Qwen-VL，采用三阶段训练方法。 1. Stage1： 专注于训练ViT组件，利用大量图像-文本对提升大型语言模型（LLM）的语义理解。 2. Stage2：全参训练，使用更广泛的数据进行更全面的学习。 3. Stage3：冻结ViT，使用指令数据集进行LLM的微调。

Stage1 预训练
包括图像-文本对、光学字符识别（OCR）数据、交错的图像-文本文章、视觉问答数据集、视频对话和图像知识数据集。数据源主要来自清理过的网页、开源数据集和合成数据，数据截止日期为2023年6月。

在初始预训练阶段，Qwen2-VL接触到约6000亿个tokens。Qwen2-VL的LLM组件使用Qwen2中的参数初始化，而视觉编码器则基于DFN的ViT初始化。原DFN ViT中的固定位置embedding被RoPE-2D替代。此阶段主要学习图像-文本关系、通过OCR进行文本内容识别、图像分类任务，为模型建立视觉-文本关联的扎实基础。

Stage2 多任务预训练
涉及额外的8000亿个与图像相关的数据。此阶段引入了更多混合图像-文本内容，促进了对视觉和文本信息之间相互作用的更细致理解。视觉问答数据集的纳入提升了模型对图像相关查询的响应能力，同时，多任务数据集的引入对模型同时处理多种任务的能力至关重要，这在处理复杂的现实世界数据集时尤为重要。纯文本数据继续在维持和提高模型语言能力方面发挥关键作用。

在整个预训练阶段，Qwen2-VL处理了总计1.4万亿个tokens，包括文本tokens和图像tokens。然而，训练过程中仅对文本tokens提供监督。

Stage3 指令微调
采用ChatML格式构建指令跟随数据集。该数据集不仅包括纯文本对话数据，还包含多模态对话数据。包括：图像问答、文档解析、多图像比较、视频理解、视频流对话和基于代理的交互。

## Temperature是怎么用的？



https://zhuanlan.zhihu.com/p/666670367

只用记住温度越大越随机，温度越小越稳定就可以了。

<img src="pic/v2-c22954bffe57a6eded5ca20d542f69fb_1440w.jpg" alt="img" style="zoom:50%;" />

具体的数学形式，就是对每一个在Softmax前prob项除以一个温度T：
$$
\begin{array}{c}
p\left(x_{i}\right)=\frac{e^{\frac{x_{i}}{T}}}{\sum_{j=1}^{V} e^{\frac{x_{j}}{T}}} \\
=\frac{1}{\sum_{j=1}^{V} e^{\frac{x_{j}-x_{i}}{T}}}
\end{array}
$$





## LLaVA 的anyres 是怎么处理的

输入的图片在get_item的时候就被送去打成patch:

在anyres的设置下, 会先找到最佳分辨率(可选择的[[336, 672], [672, 336], [672, 672], [1008, 336], [336, 1008]])

> *在选择最佳分辨率时，会考虑将原始图像隐式缩放 （通过计算缩放比例 scale 和下采样尺寸）到每个可能的候选分辨率，然后基于有效像素利用率（最大化）和像素浪费（最小化）来确定最佳选择*

然后拉伸, `image_padded = resize_and_pad_image(image, best_resolution)`

然后打成patch `patches = divide_to_patches(image_padded, processor.crop_size['height'])`, 分成多个336*336的图

![img](pic/v2-cc69ae3416f3596f04257474ec8aa565_1440w-20250312233856451.jpg)

图片经过vit, vit每一个block输出都是[5,577,1024], 第一个是5张图片, 其中1张原图, 4张patches

只取最后一个[5, 576, 1024]