

# 数学和手撕

## 常见导函数

| 表达式                    | 导数                                          |
| ------------------------- | --------------------------------------------- |
| $f(x) = x^n$              | $f'(x) = n x^{n-1}$                           |
| $f(x) = e^x$              | $f'(x) = e^x$                                 |
| $f(x) = a^x$              | $f'(x) = a^x \ln a$                           |
| $f(x) = \ln x$            | $f'(x) = \dfrac{1}{x}$                        |
| $f(x) = \log_a x$         | $f'(x) = \dfrac{1}{x \ln a}$                  |
| $f(x) = \sin x$           | $f'(x) = \cos x$                              |
| $f(x) = \cos x$           | $f'(x) = -\sin x$                             |
| $f(x) = \tan x$           | $f'(x) = \sec^2 x$                            |
| $f(x) = \arcsin x$        | $f'(x) = \dfrac{1}{\sqrt{1-x^2}}$             |
| $f(x) = \arccos x$        | $f'(x) = -\dfrac{1}{\sqrt{1-x^2}}$            |
| $f(x) = \arctan x$        | $f'(x) = \dfrac{1}{1+x^2}$                    |
| $f(x)=g(h(x))$            | $f'(x)=g'(h(x))\cdot h'(x)$                   |
| $f(x)=u(x)v(x)$           | $f'(x)=u'(x)v(x)+u(x)v'(x)$                   |
| $f(x)=\dfrac{u(x)}{v(x)}$ | $f'(x)=\dfrac{u'(x)v(x)-u(x)v'(x)}{[v(x)]^2}$ |

## 似然和概率的区别

**概率：** 是指在一个已知模型（含已知固定参数）的前提下，预测可能出现某个数据或事件的可能性。

**似然：** 是指已观测到数据后，用于描述在不同参数假设下产生该数据的“可能性”。

## 手撕 Self-attn 

```python
import torch
import torch.nn as nn

class SelfAttention(nn.Module):
    def __init__(self, embed_dim):
        """
        embed_dim: 输入特征的维度（通常也是输出特征的维度）
        """
        super(SelfAttention, self).__init__()
        self.embed_dim = embed_dim

        # 定义用于生成 query、key 和 value 的线性映射
        self.W_q = nn.Linear(embed_dim, embed_dim)
        self.W_k = nn.Linear(embed_dim, embed_dim)
        self.W_v = nn.Linear(embed_dim, embed_dim)

        # 缩放因子，通常取开方(embed_dim)用于稳定梯度
        self.scale = embed_dim ** 0.5

    def forward(self, x):
        """
        x: 张量，形状为 (batch_size, seq_length, embed_dim)
        """
        # 生成 query, key 和 value
        Q = self.W_q(x)  # (batch_size, seq_length, embed_dim)
        K = self.W_k(x)  # (batch_size, seq_length, embed_dim)
        V = self.W_v(x)  # (batch_size, seq_length, embed_dim)

        # 计算注意力分数，使用缩放点积
        # Q 与 K 的转置相乘，得到 (batch_size, seq_length, seq_length) 的分数矩阵
        attention_scores = torch.matmul(Q, K.transpose(-2, -1)) / self.scale

        # 对注意力分数做 softmax 归一化
        attention_weights = torch.softmax(attention_scores, dim=-1)

        # 注意力加权求和得到最终输出
        out = torch.matmul(attention_weights, V)  # 前面的 Batch 维度会自动对齐
        return out, attention_weights

```

## 手撕MHA (WXG, PAYPAL)

```python
import torch
import torch.nn as nn
import math
class MultiHeadAttention(nn.Module):
    def __init__(self, embed_dim, num_heads):
        """
        Args:
            embed_dim: 输入的embedding维度（也是输出维度）
            num_heads: 注意力头的数量
        """
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        # 保证 embed_dim 可以被 num_heads 整除
        assert embed_dim % num_heads == 0, "Embedding dimension must be divisible by number of heads"
        self.head_dim = embed_dim // num_heads
        # 定义三个线性层，用于映射到 query, key, value 向量
        self.q_linear = nn.Linear(embed_dim, embed_dim)
        self.k_linear = nn.Linear(embed_dim, embed_dim)
        self.v_linear = nn.Linear(embed_dim, embed_dim)
        # 输出映射层，将多个头拼接后的结果转换到最终空间
        self.out_linear = nn.Linear(embed_dim, embed_dim)
    
    def forward(self, query, key, value, mask=None):
        """
        Args:
            query, key, value: 形状均为 (batch_size, seq_len, embed_dim)
            mask: 可选参数，形状为 (batch_size, 1, 1, seq_len) 或 (batch_size, 1, seq_len, seq_len)
                  用于在计算注意力得分时将部分区域置为无效
            
        返回：
            output: 注意力模块输出，形状 (batch_size, seq_len, embed_dim)
            attn: 注意力权重，形状 (batch_size, num_heads, seq_len, seq_len)
        """
        batch_size = query.size(0)
        # 1. 线性映射，将输入映射到 query, key, value 空间
        Q = self.q_linear(query)  # (batch_size, seq_len, embed_dim)
        K = self.k_linear(key)
        V = self.v_linear(value)
        # 2. 分头：将 embed_dim 分成 num_heads 个 head，每个 head 的维度为 head_dim
        # 新的形状：(batch_size, num_heads, seq_len, head_dim)
        Q = Q.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        K = K.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        V = V.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1, 2)
        
        # 3. 计算 scaled dot-product attention
        # Q 与 K^T 的结果形状：(batch_size, num_heads, seq_len, seq_len)
        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.head_dim)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float("-inf"))
        attn = torch.softmax(scores, dim=-1)
        
        # 将注意力权重与 V 相乘，得到加权后的上下文向量
        context = torch.matmul(attn, V)  # (batch_size, num_heads, seq_len, head_dim)
        
        # 4. 拼接所有 head 的输出
        # 先将维度转回 (batch_size, seq_len, num_heads, head_dim)，再合并 num_heads 与 head_dim
        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.embed_dim)
        
        # 5. 输出层
        output = self.out_linear(context)
        return output, attn

```

## 手撕MQA

就是k和v的投影矩阵改成直接投射到head_dim, 反正后面k和v也只留一份

```python
 ## 多查询注意力
import torch
from torch import nn
class MutiQueryAttention(torch.nn.Module):
    
    def __init__(self, hidden_size, num_heads):
        super().__init__()
        self.num_heads = num_heads
        self.head_dim = hidden_size // num_heads   
        ## 初始化Q、K、V投影矩阵
        self.q_linear = nn.Linear(hidden_size, hidden_size)# 用的完整的
        self.k_linear = nn.Linear(hidden_size, self.head_dim) #这里用的是head_dim
        self.v_linear = nn.Linear(hidden_size, self.head_dim) #这里用的是head_dim
        self.o_linear = nn.Linear(hidden_size, hidden_size)
        
        
        
    def forward(self, hidden_state, attention_mask=None):
        batch_size = hidden_state.size()[0]
        
        query = self.q_linear(hidden_state)
        key = self.k_linear(hidden_state)
        value = self.v_linear(hidden_state) 
        
        query = query.view(batch_size, -1, self.num_heads, self.head_dim).transpose(1,2) # 只有q用了self.num_heads
        key = key.view(batch_size, -1, 1, self.head_dim).transpose(1,2)
        value = value.view(batch_size, -1, 1, self.head_dim).transpose(1,2)
        
        # k和v复制多头
        key = key.expand(-1, self.num_heads, -1, -1)        
        value = value.expand(-1, self.num_heads, -1, -1) 
        
        ## 计算注意力分数
        scores = torch.matmul(query, key.transpose(-1, -2)) / math.sqrt(self.head_dim)
        
        if attention_mask != None:
            scores = scores.masked_fill(attention_mask == 0, float('-inf'))
            
        ## 对注意力分数进行归一化
        att = torch.softmax(scores, dim=-1) 
        output = torch.matmul(attn, value) 
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.head_dim * self.num_heads) 
        output = self.o_linear(output) 
        return output       
```

## 手撕GQA (爱橙)

![img](https://pic3.zhimg.com/v2-6e7afe48c72b22bed524480e406ef16e_1440w.jpg)

GQA就是对q分组, 每组q共享一个k和v

```python
import torch
import torch.nn as nn
import math

class GroupQueryAttention(nn.Module):
    def __init__(self, embed_dim=512, num_heads=8, groups=2):
        super().__init__()
        # assert embed_dim % num_heads == 0, "embed_dim must be divisible by num_heads"
        # assert num_heads % groups == 0, "num_heads must be divisible by groups" 
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.groups = groups # 每一组需要一堆k和v, 这里就是需要准备多少组k和v
        self.head_dim = embed_dim // num_heads
        self.group_heads = num_heads // groups  # 多少个q共享一个k和v. 即组的大小
        
        # 注意KV的投影维度是groups * head_dim
        self.q_proj = nn.Linear(embed_dim, embed_dim) # 和MHA,MQA一样q都是不动的
        self.k_proj = nn.Linear(embed_dim, self.groups * self.head_dim)
        self.v_proj = nn.Linear(embed_dim, self.groups * self.head_dim) 
        self.out_proj = nn.Linear(embed_dim, embed_dim)

    def forward(self, x, key_padding_mask=None):
        # x: (batch_size, seq_len, embed_dim)
        batch_size, seq_len, _ = x.size()
        # 投影QKV
        q = self.q_proj(x)  # (batch, seq_len, embed_dim)
        k = self.k_proj(x)  # (batch, seq_len, groups*head_dim)
        v = self.v_proj(x)  # (batch, seq_len, groups*head_dim)
        
        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        # 这里就是对k和v先和q的分组去对应, 对应上了组内再去自我复制
        k = k.view(batch_size, seq_len, self.groups, self.head_dim).transpose(1, 2)  # (batch, groups, seq_len, head_dim)
        v = v.view(batch_size, seq_len, self.groups, self.head_dim).transpose(1, 2)  # (batch, groups, seq_len, head_dim)
        
        # 组内复制
        # unsqueeze后原本的维度后移 
        # 如果直接expand group所在的dim, 那么数据就是每个group交错的
        k = k.unsqueeze(2).expand(-1, -1, self.group_heads, -1, -1).contiguous() 
        # groups*group_heads = self.num_heads, 平铺起来方便计算
        k = k.view(batch_size, self.num_heads, seq_len, self.head_dim)
        v = v.unsqueeze(2).expand(-1, -1, self.group_heads, -1, -1).contiguous()
        v = v.view(batch_size, self.num_heads, seq_len, self.head_dim)
        
        # 计算注意力分数
        scores = torch.matmul(q, k.transpose(-1, -2))/math.sqrt(self.head_dim)  # (batch, num_heads, seq_len, seq_len)
        if key_padding_mask is not None:
            # 处理padding mask (batch, seq_len)
            mask = key_padding_mask.view(batch_size, 1, 1, seq_len)
            scores = scores.masked_fill(mask == 0, float('-inf'))
        attn = torch.softmax(scores, dim=-1)
        # 加权求和
        output = torch.matmul(attn, v)  # (batch, num_heads, seq_len, head_dim)
        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)
        output self.out_proj(output)
        return output
```





## 什么是困惑度

**定义与直观解释**
衡量语言模型对测试数据的“惊讶程度”。直观上，可以把困惑度看作模型对下一个词做出预测时的不确定性。困惑度越低，说明模型对数据的预测越“确定”，性能越好；反之，困惑度高则表明模型对数据的预测不稳定，表现较差。

**数学表达**
语言模型产生的token序列 $w_1, w_2, \dots, w_N$，模型为该序列分配的概率为 $P(w_1, w_2, \dots, w_N)$。困惑度定义为：
$$
\text{Perplexity} = \exp\left(-\frac{1}{N} \sum_{i=1}^{N} \log P(w_i \mid w_1,\dots,w_{i-1})\right)
$$


这个公式中，$-\frac{1}{N} \sum_{i=1}^{N} \log P(w_i \mid w_1,\dots,w_{i-1})$ 是平均交叉熵，故**困惑度其实是交叉熵的平均值的指数版本**。

**困惑度越小越好, 最小值为1.**

> 1. **交叉熵（Cross-Entropy）**
>    对于一个真实分布 $ p(x) $ 和模型预测的分布 $ q(x) $，交叉熵定义为：
>    $$
>    H(p, q) = \sum_x p(x) \log \frac{1}{q(x)}
>    $$
>      在语言模型中，常常假设真实分布$p(x)$ 是“one-hot”分布（即目标词的概率为 1，其它词为 0），那么对于一个正确的词 $ x $ 得到的负对数似然就是：
>    $$
>    \log \frac{1}{q(x)}
>    $$
>
>    对整个数据集取平均，就得到了每个 token 的平均交叉熵损失。
>
> 2. **困惑度（Perplexity）**
>    $$
>    \text{PPL} = \exp\left(\frac{1}{N}\sum_{i=1}^{N} -\log q(x_i)\right)
>    $$
>    其中 \( N \) 是token的数量。这个公式告诉我们：先计算所有 token 的负对数概率的平均值（也就是平均交叉熵），然后取自然指数得到困惑度。

## 熵和KL散度和交叉熵损失函数

熵的公式为 
$$
H(p) = -\sum_{i} p(i) \log p(i)
$$
直观上可以这样理解：  

1. 每个事件 $i$ 发生的概率为 $p(i)$。当一个事件发生时，如果它的概率很小，那么“惊讶”程度（或信息量）就会很大；反之，如果事件非常常见，则信息量较低。  

2. 我们用 $-\log p(i)$ 来衡量一个事件发生时的信息量（惊讶度）。概率越小，$-\log p(i)$ 就越大，说明获得的信息量越多。  

3. 熵 $H(p)$ 是所有可能事件的信息量的概率加权平均值，即我们在平均意义上需要 “描述” 这个系统中的信息量是多少。

换句话说，熵描述的是一个分布的不确定性或混乱程度。如果概率分布很均匀，熵较大；如果某个事件远比其他事件更可能发生，熵就会较低。

* * *

下面我们用类似解释熵的方式来直观理解 KL 散度。假设有两个分布 $P$ 和 $Q$，其 KL 散度的公式为  
$D_{KL}(P\parallel Q)=\sum_{i} p(i) \log \frac{p(i)}{q(i)}$  

我们可以从下面几点来理解这个公式：

1. 假设事件 $i$ 的真实概率为 $p(i)$，而在模型 $Q$ 中，事件 $i$ 的概率为 $q(i)$。当我们使用 $Q$ 来描述 $P$ 的情况时，有时会产生“错配”。  

2. 公式中的比值 $\frac{p(i)}{q(i)}$ 表示在事件 $i$ 上，真实分布和模型分布之间的差异。  

  - 如果 $p(i)$ 大于 $q(i)$，说明 $Q$ 低估了事件 $i$ 的发生概率，从而我们“失去了”一些信息；  
  - 如果 $p(i)$ 小于 $q(i)$，则 $Q$ 高估了事件 $i$ 的可能性，也会有信息损失。  

3. 对每个事件 $i$，信息量的测度为 $\log\frac{p(i)}{q(i)}$。这与熵中用 $-\log p(i)$ 来衡量信息量类似，只不过这里是比较两种概率的“惊讶程度”差值。  

  - 当 $p(i)$ 与 $q(i)$ 越接近时，$\log\frac{p(i)}{q(i)}$ 趋近于 $0$，说明 $Q$ 对于事件 $i$ 的描述与真实情况较为吻合；  
  - 当两者相差较大时，对应的值也会增大，表明此时 $Q$ “偏离”了真实分布很多。  

4. 将对每个事件 $i$ 的“额外信息量” $p(i) \log\frac{p(i)}{q(i)}$ 按照概率 $p(i)$ 做加权平均，也就是考虑在 $P$ 的世界中，每个事件发生的概率后，累加所有事件的“误差”。这就得到了 KL 散度。

5. 从另一角度看，KL 散度描述的是如果我们错误地以 $Q$ 而非 $P$ 为基础来编码或描述信息，需要额外付出的平均编码成本。也就是说，KL 散度越大，就越说明我们在用 $Q$ 来描述 $P$ 时“付出”了更多的信息量，误差也越大。

总结一下：KL 散度衡量的就是两个概率分布之间的信息差距，是按照真实分布 $P$ 中各事件发生的概率，对每个事件在用 $Q$ 与 $P$ 表示时所需额外信息量（或“惊讶感”）的平均值。因此，KL 散度可以看作是信息编码效率上的损失或不匹配程度。  

这种理解与熵的解释类似，熵描述了一个系统内在的不确定性，而 KL 散度描述了两个系统之间描述信息时可能产生的额外代价。

***

对于两个概率分布 $p$（真实分布）和 $q$（模型分布），交叉熵定义为  
$$
H(p, q) = -\sum_i p(i) \log q(i)
$$
直观上，交叉熵表示如果我们使用模型 $q$ 来编码真实分布 $p$ 的信息时，平均每个事件所需要的编码位数。相比于熵 $H(p) = -\sum_i p(i) \log p(i)$ 来衡量真实分布自身的不确定性，交叉熵在计算中使用了模型给出的概率 $q(i)$，从而隐含了描述错误的信息代价。



考虑熵与交叉熵的差值：  
$$
H(p, q) - H(p) = -\sum_i p(i) \log q(i) +\sum_i p(i) \log p(i) = \sum_i p(i) \log \frac{p(i)}{q(i)}
$$
右侧正是 KL 散度的定义  
$$
D_{\mathrm{KL}}(p\|q) = \sum_i p(i) \log \frac{p(i)}{q(i)}
$$
因此，我们就有下面的重要恒等式：  
$$
H(p, q) = H(p) + D_{\mathrm{KL}}(p\|q)
$$
这一公式有如下含义：

1. $H(p)$ 是与真实分布相关的固有不确定性，它与模型 $q$ 无关；  

2. $D_{\mathrm{KL}}(p\|q)$ 则度量了模型 $q$ 离真实分布 $p$ 的偏差，也就是使用 $q$ 进行编码时附加的“额外成本”。  

3. 使用交叉熵作为损失函数既考虑了目标的不确定性，也衡量了模型预测的准确性.

   交叉熵体现了两层信息：

   1. 真正来自于数据的内在不确定性（即熵）；
   2. 由于模型与真实分布不一致而引入的额外损失（即KL散度）。

***

【二元分类中的交叉熵损失】

对于离散的二元分类问题，我们通常设真实标签 $y\in\{0,1\}$，并令模型预测的正类概率为 $\hat{y}$（对负类的预测为 $1-\hat{y}$）。此时真实分布 $p$ 通常表示为“one-hot”形式，例如，当 $y=1$ 时，  
$$
p(1)=1,\quad p(0)=0.
$$
对应的交叉熵损失函数写为  
$$
L = -\Bigl[y\log\hat{y} + (1-y)\log(1-\hat{y})\Bigr].
$$

在二元分类的场景下，如果我们分析交叉熵与 KL 散度的关系，可以发现：

1. 当真实分布 $p$ 为 one-hot 分布时，其熵 $H(p) = -[1\cdot\log1+ 0\cdot\log0] = 0$。  **这代表着one-hot情况下, 没有内在不确定性.**

2. 因此，此时有  
   $$
   H(p, q) = H(p) + D_{\mathrm{KL}}(p\|q) = D_{\mathrm{KL}}(p\|q),
   $$

**即交叉熵损失实际上就等于 KL 散度**。这说明在这种极端情形下，优化交叉熵损失就是直接在缩小真实分布与模型分布之间的差距。

对于非 one-hot 的标签（例如软标签，soft targets）或在某些不确定性的情形下，真实分布 $p$ 本身存在熵值，此时交叉熵分解为系统固有的不确定性 $H(p)$ 和由于模型偏差带来的额外代价 $D_{\mathrm{KL}}(p\|q)$。此时最小化交叉熵仍然会促使模型分布 $q$ 逐渐接近真实分布 $p$，以降低 KL 散度贡献。

─────────────────────────────  
【总结】

- 交叉熵 $H(p, q) = -\sum_i p(i)\log q(i)$ 衡量了用模型分布 $q$ 描述真实分布 $p$ 所需要的编码成本。  
- 通过拆分公式，我们得到了 $H(p, q) = H(p) + D_{\mathrm{KL}}(p\|q)$，其中 $H(p)$ 与 $q$ 无关，$D_{\mathrm{KL}}(p\|q)$ 则是我们需要通过训练来减少的部分。  
- 在二元分类中，当真实标签是确定性的（one-hot 编码）时，熵 $H(p)$ 为零，交叉熵损失就直接等价于 KL 散度；而在一般情况下，交叉熵则包含了同描述系统固有不确定性和由模型错误所引入额外代价两个部分。

这种对交叉熵的理解不仅帮助我们更好地理解损失函数设计背后的信息论原理，也让我们明白：降低交叉熵损失的过程实质上是在减少用错误模型描述真实数据时所付出的额外编码成本。



## F1, P, R

1. 查准率（Precision）公式：

$$
\text{Precision} = \frac{TP}{TP + FP}
$$

2. 召回率（Recall）公式：

$$
\text{Recall} = \frac{TP}{TP + FN}
$$

3. F1 Score 公式（查准率和召回率的调和平均）：

$$
F1 = 2 \cdot \frac{\text{Precision} \times \text{Recall}}{\text{Precision} + \text{Recall}}
$$

## ROC和PR曲线的细节

* 对于 ROC 曲线，x 轴代表 FPR，y 轴代表 TPR，通过不同阈值的取值描绘模型在正负样本区分上的能力。(越靠近左上角越好)

<img src="pic/v2-16b05c37e967b9cfcb36059589cab80f_1440w.jpg" alt="img" style="zoom:50%;" />

- 对于 PR 曲线，x 轴代表 Recall，y 轴代表 Precision，更侧重于评价正样本的检测情况。(越靠近右上角越好)

  <img src="pic/v2-cd144689820de15fc5d5ac82bfbdf84c_1440w.jpg" alt="img" style="zoom: 50%;" />

#### ROC 曲线

- **定义：**  
  ROC 曲线展示了不同判定阈值下模型的「真正率」（True Positive Rate, TPR）与「假正率」（False Positive Rate, FPR）的权衡情况。  

- **计算方式：**  
  对于每个选择的阈值：
  - 真正率（TPR，也叫召回率）：  
    TPR = TP / (TP + FN)
  - 假正率（FPR）：  
    FPR = FP / (FP + TN)  
    其中 TP（True Positives）为真正例数，FN（False Negatives）为漏报数，FP（False Positives）为误报数，TN（True Negatives）为真负例数。

- **评价指标：**  
  ROC 曲线下面积（Area Under Curve，AUC-ROC）是一个常用的综合评价指标，值越接近 1 表示模型性能越好。

- **优缺点及适用场景：**  
  - 优点：  
    - 不受类别分布不平衡问题的影响较大，因为 FPR 考虑了 TN 数量，在类别平衡或轻微不平衡的场景下效果较好。  
    - 能直观地展示模型对不同误差类型的权衡。  
  - 缺点：  
    - 当负样本数量远大于正样本时（极度不平衡数据），FPR 可能非常低，即使有大量误报也难以反映在 ROC 上。

---

#### PR 曲线

- **定义：**  
  PR 曲线展示了不同阈值下模型的「查准率」（Precision）与「召回率」（Recall，即 TPR）的关系，更关注正样本的检测质量。

  > 在二分类任务中，模型通常会输出一个表示为正例的概率或分数，而判定一个样本属于正例还是负例需要将这个概率与一个事先设定的门槛值比较。这个门槛值就称为判定阈值。

- **计算方式：**  
  对于每个选择的阈值：

  - 查准率（Precision）：  
    Precision = TP / (TP + FP)
  - 召回率（Recall）：  
    Recall = TP / (TP + FN)

- **评价指标：**  
  常用平均精度均值（Average Precision, AP）来总结 PR 曲线的表现。AP 会综合考虑不同召回率下的查准率。

- **优缺点及适用场景：**  

  - 优点：  
    - 在正负样本分布严重不平衡的场景下表现更为敏感，因为它直接反映了误报（FP）的影响。  
    - 对于主要关注正样本检测（如罕见事件检测）的任务，PR 曲线能提供更有意义的评价。  
  - 缺点：  
    - 在类别比较平衡的情况下，PR 曲线与 ROC 曲线可能提供的信息重合较大。

***

#### 阈值选择与曲线绘制

- **阈值变化：**  
  无论是 ROC 还是 PR 曲线，都通过调整模型的决策阈值（比如将连续的预测概率转换为二分类标签的方法）来计算 TP、FP、FN、TN，从而绘制曲线。随着阈值从较高降到较低，通常：
  - TPR 与 Recall 会增加（更多正样本被预测为正）。
  - FPR 会增加，Precision 可能下降（更多负样本被误判为正）。

- **曲线形状的解释：**  
  - 对 ROC 曲线而言，理想的曲线应该越靠近左上角（低 FPR，高 TPR），而随机猜测的模型大致在对角线上。  
  - 对 PR 曲线而言，理想的曲线应尽可能高（高 Precision）且延伸到右侧（高 Recall）；随机模型的 Precision 等于正样本的比例。

***

**与类别不平衡的关系**

- 当正负样本数量非常不平衡时：
  - ROC 曲线可能显示出较高的 AUC，因为 FPR 的分母 TN 数量巨大，即使存在大量 FP，FPR 数值依然较低。  
  - PR 曲线则会更直观地反映低 Precision，因为 FP 对 Precision 的影响很大。在这种场景下，PR 曲线往往比 ROC 曲线更能揭示模型在正样本预测上的实际效能。

***

ROC 和 PR 曲线各有侧重：  

- ROC 曲线强调模型对正负样本区分的全面能力，适合类别较平衡的情况。  
- PR 曲线则更加关注正样本，特别适用于不平衡数据场景。  

在模型评估时，建议结合两者进行综合分析，同时根据实际的业务需求和样本分布情况选择合适的评估指标。

以上就是 ROC 和 PR 曲线的主要细节介绍。



# 条件概率

条件概率
$$
P(A \mid B) = \frac{P(A \cap B)}{P(B)} \quad (P(B) > 0)
$$
贝叶斯:
$$
P(A \mid B) = \frac{P(B \mid A) \cdot P(A)}{P(B)}
$$
全概率:
$$
P(B) = P(B \mid A) \cdot P(A) + P(B \mid \bar{A}) \cdot P(\bar{A})
$$








# 余弦相似度

对于两个向量 $\mathbf{A}$ 和 $\mathbf{B}$，余弦相似度计算如下：

$$
\cos(\mathbf{A}, \mathbf{B}) = \frac{\mathbf{A} \cdot \mathbf{B}}{|\mathbf{A}||\mathbf{B}|} = \frac{\sum_{i=1}^{n} A_i B_i}{\sqrt{\sum_{i=1}^{n} A_i^2} \sqrt{\sum_{i=1}^{n} B_i^2}}
$$

```python
def cosine_similarity_manual(x, y, eps=1e-8):
    # 计算点积
    dot_product = torch.sum(x * y, dim=-1)
    # 计算各自的范数
    norm_x = torch.sqrt(torch.sum(x ** 2, dim=-1))
    norm_y = torch.sqrt(torch.sum(y ** 2, dim=-1))
    # 加入 eps 防止除零错误
    cosine_sim = dot_product / (norm_x * norm_y + eps)
    return cosine_sim

cos_sim_manual = cosine_similarity_manual(vec1, vec2)
print("Cosine Similarity (Manual):", cos_sim_manual.item())
```



#  Pair-wise Contrastive Loss 

$$
L_{contrastive} = y_{ij} \cdot d_{ij}^2 + (1-y_{ij}) \cdot \max(0, m - d_{ij})^2
$$



```python
def contrastive_loss(feat1, feat2, label, margin=1.0):
    dist = F.pairwise_distance(feat1, feat2, keepdim=False)
    pos_loss = label * dist.pow(2)
    neg_loss = (1 - label) * F.relu(margin - dist).pow(2)
    return (pos_loss + neg_loss).mean()
```



# Triplet Loss


三元组损失要求对于一个锚点样本 \(a\)，存在一个正样本 \(p\)（与锚点属于同一类别）和一个负样本 \(n\)（与锚点类别不同）。目标是确保锚点与正样本之间的距离小于锚点与负样本之间的距离，并且二者之间至少有一个预先设定的“margin”。

**数学表达式：**
$$
\mathcal{L} = \max\left\{ d(a, p) - d(a, n) + \text{margin},\ 0 \right\}
$$
其中：

- $d(\cdot, \cdot)$ 表示距离度量，比如欧氏距离或余弦距离。
- $\text{margin}$ 是一个正的超参数，用于保证两类样本之间的距离差异足够大。

``` python
# ---------- 2 Triplet Loss ----------
def triplet_loss(anchor, positive, negative, margin=1.0):
    d_pos = F.pairwise_distance(anchor, positive)
    d_neg = F.pairwise_distance(anchor, negative)
    loss = F.relu(d_pos - d_neg + mar`gin)
    return loss.mean()
```

**直观理解：**
三元组损失的基本思想是：如果负样本离锚点太近，则会受到惩罚；只有当正样本与锚点足够接近、而负样本远离锚点时，才不会产生额外的损失。margin是要求再正样本和锚点再近一点, 负样本和锚点再远一点.





# InfoNCE Loss

$$
\text{InfoNCE} \ \text{Loss} = −\frac{1}{N}∑_{i=1}^N\log\frac{\exp(\frac{q_{i}⋅k_{i+}}{\tau})}{∑_{j=1}^N\exp(\frac{q_i⋅k_{j^−}}{\tau})}
$$

代表$$q_i$$的正负样本，本质上是交叉熵 loss，做的是一个 k+1 类的分类任务，目的就是想把 q 这个图片分到 k+ 这个类

```python
# ---------- 3 InfoNCE Loss ----------
def infonce_loss(embeddings, temperature=0.1):
    # embeddings: shape (2N, D) 包含 N 对正样本
    features1 = embeddings[:N]     # anchor
    features2 = embeddings[N:]     # positive
    
    logits = torch.matmul(features1, features2.T)  # 相似度矩阵 (N, N)
    logits /= temperature
    
    labels = torch.arange(N)  # 正样本在对角线
    loss = CrossEntropyLoss(logits, labels) # 这里使用的pytorch实现, 包含了softmax操作, 抵掉上面需要的exp
```

# 手撕交叉熵损失函数

交叉熵(带softmax, pytorch实现):

先对输入进来的logits(这里的$z$)用softmax(多分类)或者sigmoid(二分类)进行归一化, 然后才得到我们的概率$p$
$$
p_{i j}=\frac{\exp \left(z_{i j}\right)}{\sum_{k=1}^C \exp \left(z_{i k}\right)}
$$

``` python
def ce_loss(logits, labels):
    bs = logits.size(0)
    
    # 计算exp部分, 用于进行softmax计算
    exp_logits = torch.exp(logits)                # (N, C)
    sum_exp   = exp_logits.sum(dim=1, keepdim=True)   # (N, 1) # 因为这里要求和, 所以没办法直接用log_softmax()

	# 这里的log(exp(logits))  -> logits
    log_probs = logits - torch.log(sum_exp)   # 这里就是log(p)了    # (N, C)
    
    # 4) 取出每个样本对应的正确类的 log‑prob
    # 这里nll就是Negative Log‑Likelihood的意思
    nll = -log_probs[torch.arange(bs), labels]   # (N,) 
    
    # 5) 对所有样本求平均 → 最终的 CE loss
    loss = nll.mean()
    return loss
```

# 手撕KL散度

$$
D_{\mathrm{KL}}(p\|q) = \sum_i p(i) \log \frac{p(i)}{q(i)}
$$

q（分布q）：模型预测结果 → logits
p（分布p）：目标分布 → labels

``` python
def kl(logits, labels):  # logits是模型预测，labels是目标软标签 要记得softmax!!
    # 计算 log-softmax，数值稳定版本
    log_q = torch.log_softmax(logits, dim=1)  # log(q) - 模型预测的对数概率
    log_p = torch.log_softmax(labels, dim=1)  # log(p) - 目标分布的对数概率
    
    # 计算softmax概率
    q = torch.softmax(logits, dim=1)  # q分布 - 模型预测概率
    
    # KL(q||p) = sum(q * (log_q - log_p))
    kl = (q * (log_q - log_p)).sum(dim=1)  # 每个样本的KL散度
    
    return kl.mean()  # 返回batch的平均KL散度
```

# 手撕熵的计算

``` python
def entropy(logits):  # logits是模型预测, 要记得softmax!!
    # 计算 log-softmax，数值稳定版本
    log_q = torch.log_softmax(logits, dim=1)  # log(q) - 模型预测的对数概率
    q = torch.softmax(logits, dim=1)          # q - 模型预测的概率分布 # 要记得softmax!!
    
    # H(q) = -sum(q * log(q))
    entropy = -(q * log_q).sum(dim=1)  # 每个样本的熵
    
    return entropy.mean()  # 返回batch的平均熵
```





# 手写AUC

AUC的本质含义是：**随机取一个正样本和一个负样本，正样本分数大于负样本分数的概率**

推导:

1. 按照横轴FPR积分, 可以得到$A U C=\int_0^1 T P R d F P R$. 但FPR的取值大小是由分类阈值p决定的，当p确定时，FPR和TPR也随之确定

2. 当长方形的宽足够小时，即 $p+\Delta p$ 无限趋近于 $p$ 时，面积微分 $T P R d F P R$ 中的TPR可以认为不变，即分类阈值取 p 时的TPR，表示为 $T P R_p$
   $$
   T P R_p=\frac{T P_p}{P}=\frac{P_p}{P}
   $$
   （ $P$ 为所有正样本的数量，$P_p$ 为分类阈值取 $p$ 时的TP，即预测概率值在（ $\left.p, 1\right]$ 的正样本数量）。

3. 面积微分中的 $d F P R$ 可以表示为

$$
\begin{aligned}
& d F P R=F P R_{p+\Delta p}-F P R_p=\frac{F P_{p+\Delta p}}{N}-\frac{F P_p}{N}=\frac{1}{N}\left(F P_{p+\Delta p}-F P_p\right) \\
& =\frac{1}{N} F P_{p, p+\Delta p}=\frac{1}{N} N_{p, p+\Delta p}
\end{aligned}
$$

​	即$N_{p, p+\Delta p}$ 表示落在概率区间 $[p, p+\Delta p)$ 的负样本数量。

4. 因此可以有
   $$
   A U C=\int_0^1 T P R d F P R=\int \frac{P_p}{P} * \frac{N_{p, p+\Delta p}}{N}=\frac{1}{P * N} \int P_p * N_{p, p+\Delta p}=\frac{1}{P * N} \int_0^1 P_p d N_p
   $$
   
5. 注意  $dN_p$ 不是一个连续变化的量. 它只在阈值 `p` 恰好等于某个负样本的预测分数时，才会发生“跳跃”。如果我们将所有负样本按预测分数从低到高排序，那么每当我们“越过”一个负样本的分数时，$N_p$ 的值就会 `+1`。因此，$dN_p$ 在大多数时候是 `0`，只在遇到负样本分数的位置时，它的值才是 `1`（或者说，是落在那个极小分数区间的负样本数量）。

6. 所以在某个阈值$p$上, 我们拿出了一个负样本, 则$P_p$就是当前阈值分数超过这个负样本的正样本的综述. 整个求积分过程就是对所有负样本过一遍. 

7. 因此

$$
A U C=\frac{\sum I\left(p_{\text {正样本 }}, p_{\text {负样本 }}\right)}{P * N}
$$



但是这个公式计算复杂度太大了

所以我们优化得到下面这个公式

公式计算:

先对所有样本进行打分排序, 从小到大排(我们希望的是正样本越排后面越好)
$$
AUC=\frac{\sum_{\text {样本 }_i \in \text { 正样本 }} r_i-\frac{P *(P+1)}{2}}{P * N}
$$
注意这里求和符号是对$r_i$, 代表所有正样本的排名总和

然后$\frac{P *(P+1)}{2}$ 是等差数列求和, 代表P个正样本所能占据的 **“最差”排名之和**(即所有的正样本都小于任意一个负样本, 直接排名倒数了)

**用所有正样本的实际排名之和，减去它们可能的最差排名之和**，得到的结果恰好就是 **“正样本分数 > 负样本分数”的配对总数**。

> 正样本的实际排名和 $\sum r_i$ 超出其最差基准线 $\frac{P(P+1)}{2}$ 的部分，完全是由“它排在了多少个负样本之后（即分数比多少个负样本高）”所贡献的。

它将一个看似需要两两比较（复杂度 $O(P \times N)$）的问题，通过一次排序（复杂度 $O((P+N)\log(P+N))$）和一次简单的计算就解决了，大大提高了计算效率。

实现代码:

```python3
def cal_auc(labels, scores):

    total_num = labels.size(0)
    # rank by the socres
    samples = list(zip(scores, labels))

    rank = [(label, score) for score, label in sorted(samples)]

    pos_rank = [i+1 for i in range(len(total_num)) if rank[i][0]==1]

    pos_cnt = np.sum(labels==1)
    neg_cnt = np.sum(labels==0)

    auc = (np.sum(pos_rank) - (pos_cnt+1)*pos_cnt/2)/(pos_cnt*neg_cnt)

    return auc
```

ref: https://zhuanlan.zhihu.com/p/462734871

# 逻辑回归更新公式的推导 (筋斗云)

**先考虑前向传播的过程:**

给定第 $i$ 条样本:
$$
\mathbf{x}_i=(x_{i1},x_{i2},\dots ,x_{id})^{\mathsf T}\in\mathbb R^{d},\qquad 
y_i\in\{0,1\},
$$


我们用线性函数 $z_i=\mathbf{w}^{\mathsf T}\mathbf{x}_i$（$\mathbf w\in\mathbb R^{d}$ 为待学习的参数）来映射到 **Sigmoid**（逻辑函数）上：
$$
p_i \;=\;P(y_i=1\mid\mathbf{x}_i;\mathbf w) 
      \;=\;\sigma(z_i)
      \;=\;\frac{1}{1+e^{-z_i}} .
$$


对应的 **负类概率** 为 $1-p_i=\sigma(-z_i)$。



然后我们计算损失函数:
$$
\boxed{\,\mathcal{L}(\mathbf{w}) \;=\; -\ell(\mathbf{w})
      = -\sum_{i=1}^{N}\Bigl[
        y_i\log p_i + (1-y_i)\log(1-p_i)
      \Bigr]\, }.
$$
目标是最小化 $\mathcal{L}(\mathbf{w})$。

先对第 $i$ 条样本的损失
$$
\mathcal{L}_i(\mathbf{w}) = -\Bigl[
       y_i\log p_i + (1-y_i)\log(1-p_i)
   \Bigr]
$$
求梯度: $\displaystyle \frac{\partial \mathcal{L}_i}{\partial\mathbf{w}}$

因为 $p_i = \sigma(z_i)$ 且 $z_i = \mathbf{w}^\top \mathbf{x}_i$，先写出
$$
\frac{\partial p_i}{\partial z_i}= \sigma(z_i)\bigl(1-\sigma(z_i)\bigr)=p_i(1-p_i).
$$


而 $\displaystyle \frac{\partial z_i}{\partial \mathbf{w}} = \mathbf{x}_i$。

于是
$$
\frac{\partial p_i}{\partial \mathbf{w}} = p_i(1-p_i)\,\mathbf{x}_i.
$$


然后我们可以进一步计算
$$
\frac{\partial}{\partial\mathbf{w}}\log p_i = \frac{1}{p_i}\frac{\partial p_i}{\partial \mathbf{w}}
    = \frac{1}{p_i}\,p_i(1-p_i)\,\mathbf{x}_i
    = (1-p_i)\,\mathbf{x}_i,
$$

$$
\frac{\partial}{\partial\mathbf{w}}\log(1-p_i) = \frac{-1}{1-p_i}\frac{\partial p_i}{\partial \mathbf{w}}
    = -\frac{1}{1-p_i}\,p_i(1-p_i)\,\mathbf{x}_i
    = -p_i\,\mathbf{x}_i.
$$

最后
$$
\begin{aligned}
\frac{\partial \mathcal{L}_i}{\partial \mathbf{w}}
&= -\Bigl[
    y_i\frac{\partial}{\partial\mathbf{w}}\log p_i
    + (1-y_i)\frac{\partial}{\partial\mathbf{w}}\log(1-p_i)
  \Bigr] \\
&= -\Bigl[
    y_i (1-p_i)\mathbf{x}_i
    + (1-y_i)(-p_i)\mathbf{x}_i
  \Bigr] \\[2mm]
&= -\Bigl[
    y_i(1-p_i) - (1-y_i)p_i
  \Bigr]\mathbf{x}_i \\[2mm]
&= -\Bigl[
    y_i - y_i p_i - p_i + y_i p_i
  \Bigr]\mathbf{x}_i \\[2mm]
&= -(y_i - p_i)\mathbf{x}_i.
\end{aligned}
$$
整体梯度就是把所有样本的梯度相加：
$$
\begin{aligned}
\nabla_{\mathbf{w}}\mathcal{L}(\mathbf{w})
&= \sum_{i=1}^{N} \frac{\partial \mathcal{L}_i}{\partial \mathbf{w}} \\
&= \sum_{i=1}^{N} (p_i - y_i)\,\mathbf{x}_i .
\end{aligned}
$$


若用矩阵记号，设  

- $\mathbf{X}\in\mathbb{R}^{N\times d}$为样本矩阵（第 \(i\) 行为 $\mathbf{x}_i^\top$），  
- $\mathbf{p}= [p_1,\dots,p_N]^\top$，$\mathbf{y}= [y_1,\dots,y_N]^\top$,

则
$$
\boxed{ \displaystyle \nabla_{\mathbf{w}}\mathcal{L}
   = \mathbf{X}^\top (\mathbf{p}-\mathbf{y}) }.
$$


